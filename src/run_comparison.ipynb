{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from utils.llm_clients.cached_client import CachedLLMClient\n",
    "from utils.llm_clients.cost_monitoring import LLMClientWithCostMonitoring\n",
    "from utils.llm_clients.providers.open_ai_client import OpenAIClient\n",
    "from utils.llm_clients.providers import supported_models\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_key = os.environ[\"OPEN_AI_KEY\"]\n",
    "\n",
    "openai_client = OpenAIClient(api_key=open_ai_key, model_info=supported_models.GPT_4O)\n",
    "openai_client = CachedLLMClient(client=openai_client)\n",
    "openai_client = LLMClientWithCostMonitoring(client=openai_client)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "logging.info(\"Logging initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation\n",
    "Downloading set of articles to be used for assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import utils.wiki_parser.wiki_parser as wiki_parser\n",
    "from pprint import pprint\n",
    "from utils.storage import ArticleStorage\n",
    "import os\n",
    "\n",
    "storage = ArticleStorage()\n",
    "\n",
    "for filename in os.listdir(\"data\"):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        print(filename)\n",
    "        raw_pages = wiki_parser.extract_articles_from_file(\"data/\" + filename, output_folder=Path(\"data\"))\n",
    "        storage.save_articles(raw_pages)\n",
    "\n",
    "pages_df = storage.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtering\n",
    "\n",
    "Remove sections that don't contain text or are not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = storage.load_all()\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Linki zewnętrzne']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Zobacz też']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Bibliografia']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Przypisy']\n",
    "pages_df = pages_df.drop_duplicates()\n",
    "pages_df = pages_df.reset_index(drop=True)\n",
    "pages_df\n",
    "# pages_df.to_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.question_generation import BASE_PROMT_PL, generate_question_for_text\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "pbar = tqdm(total=len(pages_df[:3]))\n",
    "questions_column = []\n",
    "\n",
    "promt_tokens_bar = tqdm(desc=\"Promt tokens\")\n",
    "completions_tokens_bar = tqdm(desc=\"Completions tokens\")\n",
    "\n",
    "for _, row in pages_df[:3].iterrows():\n",
    "    pbar.set_description(f\"Generating questions for {row['Section Title']}\")\n",
    "    questions = generate_question_for_text(openai_client, row['Section Content'], BASE_PROMT_PL)\n",
    "    questions_column.append(questions)\n",
    "    pbar.update(1)\n",
    "    # sleep(1)\n",
    "    promt_tokens_bar.update(openai_client.promt_tokens - promt_tokens_bar.n)\n",
    "    completions_tokens_bar.update(openai_client.completion_tokens - completions_tokens_bar.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_models.caching import CachedEmbeddingModel\n",
    "from utils.embedding_models.providers import hugging_face\n",
    "from utils.embedding_models.providers import open_ai\n",
    "from utils.embedding_models.providers import supported_models\n",
    "from utils.embedding_models.schema import EmbeddingModel\n",
    "from utils.vectordb.vectordb import VectorDB\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def wrapper(model: EmbeddingModel):\n",
    "    return CachedEmbeddingModel(model)\n",
    "\n",
    "vector_db = VectorDB()\n",
    "vector_db.add_index(f\"OPENAI_SMALL\", open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_3_SMALL))\n",
    "vector_db.add_index(f\"OPENAI_LARGE\", open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_3_LARGE))\n",
    "vector_db.add_index(f\"OPENAI__ADA\", open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_ADA_002))\n",
    "vector_db.add_index(\"HF_SDADAS\", hugging_face.init_model(model_info=supported_models.ST_POLISH_PARAPHRASE_FROM_DISTILROBERTA))\n",
    "\n",
    "for index_name in tqdm(vector_db.list_indices(), desc='Testing Embedding models'):\n",
    "    vector_db.insert_texts(pages_df['Section Content'].values.tolist(), index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "pbar = tqdm(total=len(vector_db.list_indices()))\n",
    "pbar.update(1)\n",
    "for index_name in vector_db.list_indices():\n",
    "    pbar.set_description(f\"Testing {index_name}\")\n",
    "    identified_matches = []\n",
    "    for _, row in tqdm(pages_df.iterrows(), desc=f\"Testing rows for {index_name}\"):\n",
    "        matched_with_answer = 0\n",
    "        for question in row['questions']:\n",
    "            found_text = vector_db.find_text(text=question, top_k=1, index_name=index_name)[0]\n",
    "\n",
    "            if found_text == row['Section Content']:\n",
    "                matched_with_answer += 1\n",
    "        identified_matches.append(matched_with_answer)\n",
    "    pages_df[index_name] = identified_matches\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings-comparison-JqTXriCt-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
