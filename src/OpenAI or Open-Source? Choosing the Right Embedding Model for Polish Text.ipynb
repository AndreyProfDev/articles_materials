{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Google Colab\n",
    "\n",
    "To be run only in Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install python-dotenv==1.0.1\n",
    "    !pip install openai==1.56.2\n",
    "    !pip install requests==2.32.3\n",
    "    !pip install httpx==0.28.1\n",
    "    !pip install gdown\n",
    "    !pip install faiss-cpu==1.9.0\n",
    "    !pip install tiktoken==0.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download utils from git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    user = \"AndreyProfDev\"\n",
    "    repo = \"articles_materials\"\n",
    "\n",
    "    # remove local directory if it already exists\n",
    "    if os.path.isdir(repo):\n",
    "        !rm -rf {repo}\n",
    "\n",
    "    !git clone https://github.com/{user}/{repo}.git\n",
    "    !rm -rf \"{repo}/src/OpenAI or Open-Source? Choosing the Right Embedding Model for Polish Text.ipynb\"\n",
    "\n",
    "    import sys\n",
    "\n",
    "    src_dir = \"src\"\n",
    "\n",
    "    path = f\"{repo}/{src_dir}\"\n",
    "    if not path in sys.path:\n",
    "        sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and cache\n",
    "\n",
    "If you want to generate results from scratch then don't donwnload cache folder or just remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    folders_to_download = [('https://drive.google.com/uc?id=1WairnDfu6HpYuTHxwUs1rdunL498UYqD', 'cache'),\n",
    "                        ('https://drive.google.com/uc?id=14eXyre-s26vNXsh-cuzq9PvH9ZexuEIB', 'data')]\n",
    "\n",
    "    for link, folder in folders_to_download:\n",
    "        cache_folder = f'articles_materials/{folder}'\n",
    "\n",
    "        if os.path.isdir(cache_folder):\n",
    "            !rm -rf {cache_folder}\n",
    "\n",
    "        file_path = f'articles_materials/{folder}.tar.gz'\n",
    "\n",
    "        # Download the file\n",
    "        gdown.download(link, file_path, quiet=False)\n",
    "\n",
    "        # Path to the .tar.gz file\n",
    "\n",
    "        # Open the tar.gz file\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            # Extract all the contents to a directory\n",
    "            tar.extractall(path='articles_materials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Notebook\n",
    "\n",
    "- Define necessary imports\n",
    "- Initialise logging\n",
    "- Initialize environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "logging.info(\"Logging initiated\")\n",
    "\n",
    "# Set up OpenAI API\n",
    "open_ai_key = os.environ[\"OPEN_AI_KEY\"]\n",
    "\n",
    "# Define paths\n",
    "data_path = \"articles_materials/data\"\n",
    "cache_path = \"articles_materials/cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display architecture of selected open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.embedding_models.providers.supported_models as supported_models\n",
    "\n",
    "logging.getLogger(\"sentence_transformers.SentenceTransformer\").setLevel(logging.WARNING)\n",
    "\n",
    "# embedding_model = SentenceTransformer(supported_models.ST_POLISH_PARAPHRASE_FROM_MPNET.model_name)\n",
    "# embedding_model = SentenceTransformer(supported_models.ORB_ST_POLISH_KARTONBERTA_BASE_ALPHA_V1.model_name)\n",
    "# embedding_model = SentenceTransformer(supported_models.ORB_KARTONBERT_USE.model_name)\n",
    "\n",
    "embedding_model = SentenceTransformer(supported_models.ST_POLISH_PARAPHRASE_FROM_DISTILROBERTA.model_name)\n",
    "num_params = sum(p.numel() for p in embedding_model.parameters())\n",
    "\n",
    "embedding_model.bfloat16()\n",
    "\n",
    "print(\"Number of parameters: \", num_params, \"\\n\")\n",
    "\n",
    "print(\"Model architecture:\\n\" + str(embedding_model))\n",
    "\n",
    "# Move to CPU to free up GPU memory\n",
    "embedding_model.to(\"cpu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Open AI client\n",
    "\n",
    "Open AI client will be used to augment wiki data with generated question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.llm_clients.cached_client import CachedLLMClient\n",
    "from utils.llm_clients.cost_monitoring import LLMClientWithCostMonitoring\n",
    "from utils.llm_clients.providers.open_ai_client import OpenAIClient\n",
    "from utils.llm_clients.providers import supported_models\n",
    "\n",
    "openai_client = OpenAIClient(api_key=open_ai_key, model_info=supported_models.GPT_4O)\n",
    "openai_client = CachedLLMClient(client=openai_client, path_to_cache = Path(f\"{cache_path}/completion_cache\"))\n",
    "openai_client = LLMClientWithCostMonitoring(client=openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation\n",
    "\n",
    "Process mediawiki xml files downloaded from wikipedia:\n",
    "1) Extract text from xml\n",
    "2) Clean text\n",
    "3) Split text by sections\n",
    "\n",
    "All intermediate data will be stored in the following subfolders:\n",
    "- _0_raw files_ - contains original xml files\n",
    "- _1_extracted_pages_ - contains yaml files with extracted text\n",
    "- _2_processed_html_pages_ - contains yaml files with text after removing html tags\n",
    "- _3_processed_markdown_pages_ - contains yaml files with text after removing markdown tags\n",
    "- _4_split_sections_ - contains yaml files with text split by sections\n",
    "- _5_remove_empty_articles_ - contains yaml files after removing empty articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import utils.wiki_parser.wiki_parser as wiki_parser\n",
    "from pprint import pprint\n",
    "from utils.storage import ArticleStorage\n",
    "import os\n",
    "\n",
    "storage = ArticleStorage()\n",
    "\n",
    "data_files = os.listdir(f\"{data_path}/0_raw files\")\n",
    "pbar = tqdm(total = len(data_files))\n",
    "for filename in data_files:\n",
    "    pbar.set_description(f'Processing {filename}')\n",
    "    if filename.endswith(\".xml\"):\n",
    "        raw_pages = wiki_parser.extract_articles_from_file(f\"{data_path}/0_raw files/\" + filename, output_folder=Path(data_path))\n",
    "        storage.save_articles(raw_pages)\n",
    "    pbar.update(1)\n",
    "\n",
    "pages_df = storage.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtering\n",
    "\n",
    "1) Load data from vector storage as pandas dataframe\n",
    "2) Remove sections that don't contain text or sections with text just listing references\n",
    "3) Build _Section With Context_ column by adding article title and section title to the section text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = storage.load_all()\n",
    "\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Linki zewnętrzne']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Zobacz też']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Bibliografia']\n",
    "pages_df = pages_df[pages_df['Section Title'] != 'Przypisy']\n",
    "\n",
    "pages_df['Section With Context'] = pages_df['Article Title'] + '\\n' + pages_df['Section Title'] + '\\n' + pages_df['Section Content']\n",
    "pages_df.loc[pages_df['Section Title'] == 'Main', 'Section With Context'] = pages_df['Article Title'] + '\\n' + pages_df['Section Content']\n",
    "\n",
    "pages_df = pages_df.drop_duplicates()\n",
    "pages_df = pages_df.reset_index(drop=True)\n",
    "\n",
    "pages_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Generate artificial questions for each section using Open AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.question_generation import BASE_PROMT_PL, generate_question_for_text\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "pbar = tqdm(total=len(pages_df))\n",
    "questions_column = []\n",
    "\n",
    "promt_tokens_bar = tqdm(desc=\"Promt cost ($): \")\n",
    "completions_tokens_bar = tqdm(desc=\"Completions cost ($):\")\n",
    "\n",
    "for _, row in pages_df.iterrows():\n",
    "    pbar.set_description(f\"Generating questions for {row['Article Title']}/{row['Section Title']}\")\n",
    "    \n",
    "    questions = generate_question_for_text(openai_client, row['Section With Context'], BASE_PROMT_PL)\n",
    "    questions_column.append(questions.questions)\n",
    "\n",
    "    pbar.update(1)\n",
    "    promt_tokens_bar.update(openai_client.get_total_promt_cost() - promt_tokens_bar.n)\n",
    "    completions_tokens_bar.update(openai_client.get_total_completion_cost() - completions_tokens_bar.n)\n",
    "\n",
    "pages_df['questions'] = questions_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_models.providers import hugging_face\n",
    "from utils.embedding_models.providers import open_ai\n",
    "from utils.embedding_models.providers import supported_models\n",
    "from utils.vectordb.vectordb import VectorDB\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "path_to_cache = Path(f\"{cache_path}/embeddings_cache\")\n",
    "\n",
    "# Define embedding models\n",
    "embedding_models = {\n",
    "    \"HF_SDADAS\": hugging_face.init_model(model_info=supported_models.ST_POLISH_PARAPHRASE_FROM_DISTILROBERTA, path_to_cache=path_to_cache),\n",
    "    \"OPENAI_SMALL\": open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_3_SMALL, path_to_cache=path_to_cache),\n",
    "    \"OPENAI_LARGE\": open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_3_LARGE, path_to_cache=path_to_cache),\n",
    "    \"OPENAI__ADA\": open_ai.init_model(api_key=open_ai_key, model_info=supported_models.TEXT_EMBEDDING_ADA_002, path_to_cache=path_to_cache),\n",
    "    \"HF_MPNET\": hugging_face.init_model(model_info=supported_models.ST_POLISH_PARAPHRASE_FROM_MPNET, path_to_cache=path_to_cache),\n",
    "    \"HF_KARTONBERTA\": hugging_face.init_model(model_info=supported_models.ORB_ST_POLISH_KARTONBERTA_BASE_ALPHA_V1, path_to_cache=path_to_cache),\n",
    "    \"ORB_KARTONBERT_USE\": hugging_face.init_model(model_info=supported_models.ORB_KARTONBERT_USE),\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize vector database and add indices (one per each embedding model)\n",
    "vector_db = VectorDB()\n",
    "for index_name, index in embedding_models.items():\n",
    "    vector_db.add_index(index_name, index)\n",
    "\n",
    "# Populate each index with data\n",
    "sections = pages_df['Section With Context'].values.tolist()\n",
    "for index_name in vector_db.list_indices():\n",
    "    cost_bar = tqdm(desc=f'{index_name}. Total cost ($)')\n",
    "    model = embedding_models[index_name]\n",
    "    for text in tqdm(sections, desc=f'{index_name}. Processed items'):\n",
    "        vector_db.insert_text(text, index_name)\n",
    "        cost_bar.update(model.get_total_cost() - cost_bar.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chart_utils import draw_bar_chart\n",
    "\n",
    "embeddings_costs = [{\"Model\" : model.model_info.model_name, \"Cost ($)\": model.get_total_cost()} for model in embedding_models.values()]\n",
    "embeddings_costs_df = pd.DataFrame(embeddings_costs)\n",
    "embeddings_costs_df = embeddings_costs_df.sort_values(\"Cost ($)\", ascending=False)\n",
    "embeddings_costs_df = embeddings_costs_df.set_index(\"Model\")\n",
    "embeddings_costs_df = embeddings_costs_df[embeddings_costs_df[\"Cost ($)\"] > 0]\n",
    "\n",
    "display(embeddings_costs_df)\n",
    "\n",
    "draw_bar_chart(df=embeddings_costs_df, title='Total cost to generate embeddings ($)', bar_label_format = \"%.2f $\", \n",
    "               grey_colors=['#80C4E9'], highlight_colors=['#80C4E9'], numeric_labels_padding = -40, size=(8, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embedding_models.providers.supported_models import ORB_KARTONBERT_USE\n",
    "from chart_utils import draw_bar_chart\n",
    "\n",
    "embeddings_time = [{\"Model\" : model.model_info.model_name, \"Total time\": model.get_total_time()} for model in embedding_models.values()]\n",
    "embeddings_time_df = pd.DataFrame(embeddings_time)\n",
    "embeddings_time_df = embeddings_time_df.sort_values(by=\"Total time\", ascending=False)\n",
    "embeddings_time_df = embeddings_time_df.set_index(\"Model\")\n",
    "display(embeddings_time_df)\n",
    "\n",
    "draw_bar_chart(df=embeddings_time_df, title='Total time to generate embeddings (seconds)', \n",
    "               bar_label_format = \"%d s\", grey_colors=['#CDCDCD'], highlight_colors=['#80C4E9'], numeric_labels_padding=5, \n",
    "               to_highlight=[ORB_KARTONBERT_USE.model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "test_df = pages_df.copy() #[:800]\n",
    "\n",
    "evaluatiion_results = []\n",
    "\n",
    "for index_name in vector_db.list_indices():\n",
    "    model = embedding_models[index_name]\n",
    "    cost = model.get_total_cost()\n",
    "    embedding_tokens_bar = tqdm(desc=f\"{index_name}. Embedding cost ($): \")\n",
    "\n",
    "    for k in [1, 5, 10]:\n",
    "        for _, row in tqdm(list(test_df.iterrows()), desc=f\"{index_name}. k={k}. Wiki sections: \"):\n",
    "            matched_with_answer = 0\n",
    "            for question in row['questions']:\n",
    "\n",
    "                found_text = vector_db.find_text(text=question, top_k=k, index_name=index_name)\n",
    "                \n",
    "                embedding_tokens_bar.update(model.get_total_cost() - cost - embedding_tokens_bar.n)\n",
    "                evaluatiion_results.append({\n",
    "                    \"Model Name\": model.model_info.model_name,\n",
    "                    \"k\": k,\n",
    "                    \"Section\": row['Section With Context'],\n",
    "                    \"Question\": question,\n",
    "                    \"Found text\": found_text,\n",
    "                    \"Matched with answer\": row['Section With Context'] in found_text\n",
    "                })\n",
    "\n",
    "evaluatiion_results_df = pd.DataFrame(evaluatiion_results)\n",
    "evaluatiion_results_df = evaluatiion_results_df.pivot_table(index=['Section', 'Question', 'Model Name'], columns=['k'], values=['Matched with answer'], aggfunc='sum').add_prefix('k=')\n",
    "evaluatiion_results_df = evaluatiion_results_df.droplevel(0, axis=1).reset_index()\n",
    "evaluatiion_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average number of questions correctly answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_per_section_df = evaluatiion_results_df.groupby(['Section', 'Model Name'])[['k=1', 'k=5', 'k=10']].sum()\n",
    "meand_qs_df = questions_per_section_df.groupby('Model Name').mean().reset_index()\n",
    "meand_qs_df.set_index('Model Name', inplace=True)\n",
    "meand_qs_df = meand_qs_df.sort_values(by='k=1', ascending=True)\n",
    "\n",
    "print('Mean number of questions matched with answer')\n",
    "display(meand_qs_df.head())\n",
    "\n",
    "meand_qs_df.columns = ['Top 1 record', 'Top 5 records', 'Top 10 records']\n",
    "draw_bar_chart(meand_qs_df, title=\"Average # of questions matched with the answer\", to_highlight=[ORB_KARTONBERT_USE.model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of questions correctly answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_qs_df = evaluatiion_results_df.groupby(['Model Name'])[['k=1', 'k=5', 'k=10']].sum()\n",
    "total_qs_df = total_qs_df.sort_values(by='k=1', ascending=True)\n",
    "\n",
    "display(total_qs_df)\n",
    "\n",
    "total_qs_df.columns = ['Top 1 record', 'Top 5 records', 'Top 10 records']\n",
    "draw_bar_chart(total_qs_df, title=\"Total # of questions matched with the answer\", \n",
    "               bar_label_format=\"%d\", \n",
    "               to_highlight=[ORB_KARTONBERT_USE.model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings-comparison-JqTXriCt-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
